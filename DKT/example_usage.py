#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DKT ç¯å¢ƒä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ DKT ç¯å¢ƒè¿›è¡Œå­¦ä¹ è·¯å¾„æ¨è
"""
import numpy as np
from kt_env import KTEnv


def example_1_basic_usage():
    """ç¤ºä¾‹ 1: åŸºæœ¬ä½¿ç”¨æµç¨‹"""
    print("=" * 60)
    print("ç¤ºä¾‹ 1: åŸºæœ¬ä½¿ç”¨æµç¨‹")
    print("=" * 60)
    
    # 1. åˆ›å»ºç¯å¢ƒ
    env = KTEnv(model_name='DKT', dataset_name='assist09')
    
    # 2. è®¾ç½®å­¦ä¹ åœºæ™¯
    batch_size = 1
    targets = np.array([[100, 200, 300]])  # 3 ä¸ªç›®æ ‡çŸ¥è¯†ç‚¹
    initial_logs = np.array([[50, 51, 52, 53, 54]])  # 5 æ¡å†å²è®°å½•
    
    print(f"\nå­¦ä¹ ç›®æ ‡: {targets[0]}")
    print(f"åˆå§‹å†å²: {initial_logs[0]}")
    
    # 3. é‡ç½®ç¯å¢ƒ
    state = env.reset(targets, initial_logs)
    print(f"\nåˆå§‹æŒæ¡åº¦: {state['initial_score'][0]:.4f}")
    
    # 4. æ‰§è¡Œå­¦ä¹ è·¯å¾„
    learning_path = np.array([[101, 102, 103, 201, 202, 203, 301, 302, 303, 104]]).T
    print(f"\nå­¦ä¹ è·¯å¾„: {learning_path.T[0]}")
    
    for i, kc in enumerate(learning_path):
        step_info = env.step(kc.reshape(1, 1))
        print(f"  Step {i+1}: å­¦ä¹  KC {kc[0]:3d} â†’ æŒæ¡åº¦ {step_info['current_target_score'][0]:.4f}")
    
    # 5. è®¡ç®—æœ€ç»ˆç»“æœ
    final_score = env.evaluate()[0]
    reward = env.get_reward(full_score=3)[0]
    
    print(f"\næœ€ç»ˆæŒæ¡åº¦: {final_score:.4f}")
    print(f"å­¦ä¹ å¢ç›Š: {final_score - state['initial_score'][0]:+.4f}")
    print(f"å½’ä¸€åŒ–å¥–åŠ±: {reward:.4f}")


def example_2_compare_paths():
    """ç¤ºä¾‹ 2: æ¯”è¾ƒä¸åŒå­¦ä¹ è·¯å¾„çš„æ•ˆæœ"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 2: æ¯”è¾ƒä¸åŒå­¦ä¹ è·¯å¾„")
    print("=" * 60)
    
    env = KTEnv(model_name='DKT', dataset_name='assist09')
    
    # è®¾ç½®ç›¸åŒçš„åˆå§‹æ¡ä»¶
    targets = np.array([[1000, 2000, 3000]])
    initial_logs = np.array([[100, 101, 102, 103, 104]])
    
    # è·¯å¾„ A: å¾ªåºæ¸è¿›ï¼ˆæ¥è¿‘ç›®æ ‡ï¼‰
    path_a = np.array([[950, 980, 990, 995, 1000,
                        1950, 1980, 1990, 1995, 2000,
                        2950, 2980, 2990, 2995, 3000]]).T
    
    # è·¯å¾„ B: éšæœºé€‰æ‹©
    np.random.seed(42)
    path_b = np.random.randint(0, env.skill_num, (15, 1))
    
    # è¯„ä¼°è·¯å¾„ A
    state = env.reset(targets, initial_logs)
    env.step(path_a)
    reward_a = env.get_reward(full_score=3)[0]
    
    # è¯„ä¼°è·¯å¾„ B
    env.reset(targets, initial_logs)
    env.step(path_b)
    reward_b = env.get_reward(full_score=3)[0]
    
    print(f"\nè·¯å¾„ Aï¼ˆå¾ªåºæ¸è¿›ï¼‰:")
    print(f"  å¥–åŠ±: {reward_a:.4f}")
    
    print(f"\nè·¯å¾„ Bï¼ˆéšæœºé€‰æ‹©ï¼‰:")
    print(f"  å¥–åŠ±: {reward_b:.4f}")
    
    print(f"\nè·¯å¾„ A æ¯”è·¯å¾„ B {('æ›´å¥½' if reward_a > reward_b else 'æ›´å·®')}")
    print(f"å·®å¼‚: {abs(reward_a - reward_b):.4f}")


def example_3_batch_students():
    """ç¤ºä¾‹ 3: æ‰¹é‡å¤„ç†å¤šä¸ªå­¦ç”Ÿ"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 3: æ‰¹é‡å¤„ç†å¤šä¸ªå­¦ç”Ÿ")
    print("=" * 60)
    
    env = KTEnv(model_name='DKT', dataset_name='assist09')
    
    # ä¸º 5 ä¸ªå­¦ç”Ÿç”Ÿæˆä¸åŒçš„å­¦ä¹ åœºæ™¯
    batch_size = 5
    np.random.seed(123)
    
    # æ¯ä¸ªå­¦ç”Ÿæœ‰ä¸åŒçš„ç›®æ ‡
    targets = np.random.randint(0, env.skill_num, (batch_size, 3))
    
    # æ¯ä¸ªå­¦ç”Ÿæœ‰ä¸åŒçš„å†å²
    initial_logs = np.random.randint(0, env.skill_num, (batch_size, 8))
    
    print(f"\n{batch_size} ä¸ªå­¦ç”Ÿçš„å­¦ä¹ åœºæ™¯:")
    state = env.reset(targets, initial_logs)
    
    for i in range(batch_size):
        print(f"  å­¦ç”Ÿ {i+1}: ç›®æ ‡ {targets[i]}, åˆå§‹æŒæ¡åº¦ {state['initial_score'][i]:.4f}")
    
    # æ‰§è¡Œç›¸åŒçš„å­¦ä¹ è·¯å¾„ï¼ˆç®€åŒ–ï¼‰
    learning_path = np.random.randint(0, env.skill_num, (batch_size, 12))
    env.step(learning_path)
    
    # è®¡ç®—ç»“æœ
    rewards = env.get_reward(full_score=3)
    
    print(f"\nå­¦ä¹ ç»“æœ:")
    for i in range(batch_size):
        print(f"  å­¦ç”Ÿ {i+1}: å¥–åŠ± {rewards[i]:.4f}")
    
    print(f"\nå¹³å‡å¥–åŠ±: {rewards.mean():.4f}")


def example_4_real_student():
    """ç¤ºä¾‹ 4: ä½¿ç”¨çœŸå®å­¦ç”Ÿæ•°æ®"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 4: åŸºäºçœŸå®å­¦ç”Ÿæ•°æ®")
    print("=" * 60)
    
    env = KTEnv(model_name='DKT', dataset_name='assist09')
    
    # è·å–çœŸå®å­¦ç”Ÿçš„æ•°æ®
    student_id = 0
    student_data = env.get_student_data(student_id)
    
    print(f"\nçœŸå®å­¦ç”Ÿ {student_id}:")
    print(f"  å†å²é•¿åº¦: {student_data['length']}")
    print(f"  æŠ€èƒ½åºåˆ—å‰ 10: {student_data['skill_sequence'][:10]}")
    
    # ä½¿ç”¨å­¦ç”Ÿçš„å‰ 10 æ¡è®°å½•ä½œä¸ºåˆå§‹å†å²
    initial_logs = np.array([student_data['skill_sequence'][:10]])
    
    # ä»å­¦ç”Ÿçš„æŠ€èƒ½åºåˆ—ä¸­é€‰æ‹© 3 ä¸ªä½œä¸ºç›®æ ‡
    targets = np.array([student_data['skill_sequence'][10:13]])
    
    print(f"\nå­¦ä¹ ç›®æ ‡: {targets[0]}")
    
    # é‡ç½®ç¯å¢ƒ
    state = env.reset(targets, initial_logs)
    print(f"åˆå§‹æŒæ¡åº¦: {state['initial_score'][0]:.4f}")
    
    # ä½¿ç”¨å­¦ç”Ÿå®é™…å­¦ä¹ çš„ä¸‹ 10 ä¸ªæŠ€èƒ½
    actual_path = np.array([student_data['skill_sequence'][13:23]]).T
    env.step(actual_path)
    
    reward = env.get_reward(full_score=3)[0]
    print(f"å­¦ç”Ÿå®é™…è·¯å¾„çš„å¥–åŠ±: {reward:.4f}")


def example_5_adaptive_learning():
    """ç¤ºä¾‹ 5: è‡ªé€‚åº”å­¦ä¹ è·¯å¾„ï¼ˆæ ¹æ®æŒæ¡åº¦åŠ¨æ€è°ƒæ•´ï¼‰"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 5: è‡ªé€‚åº”å­¦ä¹ è·¯å¾„")
    print("=" * 60)
    
    env = KTEnv(model_name='DKT', dataset_name='assist09')
    
    # è®¾ç½®å­¦ä¹ ç›®æ ‡
    targets = np.array([[500, 1000, 1500]])
    initial_logs = np.array([[10, 20, 30, 40, 50]])
    
    state = env.reset(targets, initial_logs)
    print(f"\nåˆå§‹æŒæ¡åº¦: {state['initial_score'][0]:.4f}")
    print(f"å­¦ä¹ ç›®æ ‡: {targets[0]}")
    
    print(f"\nè‡ªé€‚åº”å­¦ä¹ è¿‡ç¨‹:")
    
    # æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„è‡ªé€‚åº”ç­–ç•¥ï¼š
    # å¦‚æœæŒæ¡åº¦ä½ï¼Œå­¦ä¹ ç›¸å…³åŸºç¡€çŸ¥è¯†ï¼›å¦‚æœæŒæ¡åº¦é«˜ï¼Œå­¦ä¹ è¿›é˜¶çŸ¥è¯†
    for step in range(10):
        current_score = env.evaluate()[0]
        
        # ç®€å•ç­–ç•¥ï¼šæ ¹æ®å½“å‰æŒæ¡åº¦é€‰æ‹©çŸ¥è¯†ç‚¹
        if current_score < 0.3:
            # æŒæ¡åº¦ä½ï¼Œå­¦ä¹ åŸºç¡€çŸ¥è¯†ï¼ˆæ¥è¿‘ç›®æ ‡ä½†æ›´ç®€å•ï¼‰
            kc = targets[0, step % 3] - 50
        elif current_score < 0.6:
            # æŒæ¡åº¦ä¸­ç­‰ï¼Œå­¦ä¹ æ ¸å¿ƒçŸ¥è¯†ï¼ˆç›®æ ‡æœ¬èº«ï¼‰
            kc = targets[0, step % 3]
        else:
            # æŒæ¡åº¦é«˜ï¼Œå­¦ä¹ è¿›é˜¶çŸ¥è¯†ï¼ˆè¶…è¿‡ç›®æ ‡ï¼‰
            kc = targets[0, step % 3] + 50
        
        kc = max(0, min(kc, env.skill_num - 1))  # ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
        
        step_info = env.step(np.array([[kc]]))
        new_score = step_info['current_target_score'][0]
        
        print(f"  Step {step+1}: æŒæ¡åº¦ {current_score:.4f} â†’ å­¦ä¹  KC {kc:4d} â†’ æ–°æŒæ¡åº¦ {new_score:.4f}")
    
    reward = env.get_reward(full_score=3)[0]
    print(f"\næœ€ç»ˆå¥–åŠ±: {reward:.4f}")


if __name__ == '__main__':
    print("\n" + "ğŸ“š " * 20)
    print(" " * 20 + "DKT ç¯å¢ƒä½¿ç”¨ç¤ºä¾‹")
    print("ğŸ“š " * 20 + "\n")
    
    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    example_1_basic_usage()
    example_2_compare_paths()
    example_3_batch_students()
    example_4_real_student()
    example_5_adaptive_learning()
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("=" * 60)

